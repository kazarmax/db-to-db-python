{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to postgresql db using sqlalchemy library and read data from it to pandas dataframe\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "postgresql_user = 'postgres'\n",
    "postgresql_password = '1'\n",
    "postgresql_host = 'localhost'\n",
    "postgresql_port = '5433'\n",
    "postgresql_dbname = 'sales'\n",
    "\n",
    "#database_uri = 'postgresql+psycopg2://user:password@host:port/dbname'\n",
    "\n",
    "database_uri = f'postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_dbname}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(database_uri)\n",
    "\n",
    "# Define the sql query\n",
    "query = 'SELECT * FROM migration.sales_data --LIMIT 100;'\n",
    "\n",
    "# Use Pandas to read the SQL query into a DataFrame (57.8 sec)\n",
    "sales_data_df = pd.read_sql(query, engine) \n",
    "\n",
    "# Print dataframe\n",
    "sales_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new field \"Shipping duration\" to calculate the difference between ship_date and order_date\n",
    "\n",
    "## Create a new dataframe that will store the new field\n",
    "updated_sales_data_df = sales_data_df.copy(deep=True)\n",
    "\n",
    "## Convert the order_date and ship_date columns to datetime\n",
    "updated_sales_data_df['order_date'] = pd.to_datetime(updated_sales_data_df['order_date'])\n",
    "updated_sales_data_df['ship_date'] = pd.to_datetime(updated_sales_data_df['ship_date'])\n",
    "\n",
    "## Calculate the difference between ship_date and order_date\n",
    "updated_sales_data_df['shipping_duration'] = (updated_sales_data_df['ship_date'] - updated_sales_data_df['order_date']).dt.days\n",
    "\n",
    "## Print the updated dataframe (see the last column)\n",
    "updated_sales_data_df\n",
    "\n",
    "## Send back updated dataframe to Postgres to the table `updated_sales_data` (9 mins)\n",
    "#updated_sales_data_df.to_sql('updated_sales_data', con=engine, schema='migration', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk_size=10000 and max_workers=4 - 4 min 30 sec\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "# Function to write chunks to SQL Server\n",
    "def write_to_sql(chunk):\n",
    "    try:\n",
    "        chunk.to_sql(name='updated_sales_data', con=engine, schema='migration', if_exists='append', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Split DataFrame into chunks (number of rows)\n",
    "chunk_size = 10000\n",
    "chunks = [updated_sales_data_df[i:i+chunk_size] for i in range(0, updated_sales_data_df.shape[0], chunk_size)] \n",
    "\n",
    "# Use ThreadPoolExecutor to write chunks in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:  # Adjust max_workers as needed\n",
    "    # Submit all the tasks and keep track of the futures\n",
    "    futures = [executor.submit(write_to_sql, chunk) for chunk in chunks]\n",
    "\n",
    "    # Using as_completed to ensure all futures are done and to handle exceptions\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            # result() will re-raise any exception caught during execution.\n",
    "            future.result()\n",
    "        except Exception as e:\n",
    "            # Handle or log the exception if needed\n",
    "            print(f\"An error occurred during database operation: {e}\")\n",
    "\n",
    "# At this point, all futures have completed, and all chunks have been processed\n",
    "print(\"All tasks are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to snowflake  using snowflake connector and read data from it to pandas dataframe\n",
    "\n",
    "import snowflake.connector \n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "          user='user',\n",
    "          password='password',\n",
    "          account='snowflake_account',\n",
    "          warehouse='WH_SUPERSTORE',\n",
    "          database='RAW',\n",
    "          schema='SUPERSTORE')\n",
    "\n",
    "# Create a cursor object.\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Execute a statement that will generate a result set.\n",
    "sql = \"select * from orders limit 100\"\n",
    "cur.execute(sql)\n",
    "\n",
    "# Fetch the result set from the cursor and deliver it as the pandas DataFrame.\n",
    "df = cur.fetch_pandas_all()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Migrate data from PostgreSQL database table to Snowflake using Pandas dataframe as intermediary \n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import snowflake.connector \n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "postgresql_user     = 'postgres'\n",
    "postgresql_password = '1'\n",
    "postgresql_host     = 'localhost'\n",
    "postgresql_port     = '5433'\n",
    "postgresql_dbname   = 'sales'\n",
    "\n",
    "# Create connection string for PostgreSQL\n",
    "database_uri = f'postgresql+psycopg2://{postgresql_user}:{postgresql_password}@{postgresql_host}:{postgresql_port}/{postgresql_dbname}'\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(database_uri)\n",
    "\n",
    "# Define the sql query\n",
    "query = 'SELECT * FROM migration.sales_data -- LIMIT 3000000;'\n",
    "\n",
    "# Use Pandas to read the SQL query into a DataFrame\n",
    "sales_data_df = pd.read_sql(query, engine)\n",
    "print(\"# of rows in sales_data_df = \", len(sales_data_df))\n",
    "\n",
    "# Configure connection to Snowflake\n",
    "snowflake_conn = snowflake.connector.connect(\n",
    "          user='user',\n",
    "          password='password',\n",
    "          account='snowflake_account',\n",
    "          warehouse='WH_SUPERSTORE',\n",
    "          database='<your_name>_MIGRATION', #Put your db name here\n",
    "          schema='RAW')\n",
    "\n",
    "success, nchunks, nrows, _ = write_pandas(snowflake_conn, sales_data_df, 'SALES_DATA4',  auto_create_table=True)\n",
    "print(f'success = {success}, nchunks = {nchunks}, nrows = {nrows}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
